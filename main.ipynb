{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d10227f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "81565ee3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import qdrant_client\n",
    "\n",
    "# Collections in Qdrant are like tables in databases, where each collection can hold a set of vectors. \n",
    "# Here, \"chat_with_docs\" is intended to store document embeddings to support query-based information retrieva\n",
    "collection_name=\"chat_with_docs\"\n",
    "\n",
    "# below initializes a QdrantClient instance, connecting it to a Qdrant server running locally.\n",
    "client = qdrant_client.QdrantClient(\n",
    "    host=\"localhost\",\n",
    "    port=6333\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e79a4b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#SimpleDirectoryReader scans a directory, filters for specific file types, and loads document content into a format we can work with.\n",
    "from llama_index.core import SimpleDirectoryReader \n",
    "\n",
    "input_dir_path = './docs'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".pdf\"],\n",
    "            recursive=True\n",
    "        )\n",
    "\n",
    "#load_data() method is used to read the PDF file’s content and return it in a structured format, storing it in docs list\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9190b2ba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(list, 32)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(docs), len(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "582f52f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Preprint\\nDSP Y: C OMPILING DECLARATIVE LANGUAGE\\nMODEL CALLS INTO SELF -IMPROVING PIPELINES\\nOmar Khattab,1 Arnav Singhvi,2\\nParidhi Maheshwari,4 Zhiyuan Zhang,1\\nKeshav Santhanam,1 Sri Vardhamanan,6 Saiful Haq,6\\nAshutosh Sharma,6 Thomas T. Joshi,7 Hanna Moazam,8\\nHeather Miller,3,9 Matei Zaharia,2 Christopher Potts1\\n1Stanford University, 2UC Berkeley, 3Carnegie Mellon University,\\n4Amazon Alexa AI, 5Dashworks Technologies, Inc.,\\n6IIT Bombay, 7Calera Capital, 8Microsoft, 9Two Sigma Investments\\nokhattab@cs.stanford.edu\\nABSTRACT\\nThe ML community is rapidly exploring techniques for prompting language mod-\\nels (LMs) and for stacking them into pipelines that solve complex tasks. Un-\\nfortunately, existing LM pipelines are typically implemented using hard-coded\\n“prompt templates”, i.e. lengthy strings discovered via trial and error. Toward a\\nmore systematic approach for developing and optimizing LM pipelines, we intro-\\nduce DSPy, a programming model that abstracts LM pipelines astext transforma-\\ntion graphs, i.e. imperative computation graphs where LMs are invoked through\\ndeclarative modules. DSPy modules are parameterized, meaning they can learn\\n(by creating and collecting demonstrations) how to apply compositions of prompt-\\ning, finetuning, augmentation, and reasoning techniques. We design a compiler\\nthat will optimize any DSPy pipeline to maximize a given metric. We conduct\\ntwo case studies, showing that succinct DSPy programs can express and optimize\\nsophisticated LM pipelines that reason about math word problems, tackle multi-\\nhop retrieval, answer complex questions, and control agent loops. Within minutes\\nof compiling, a few lines of DSPy allow GPT-3.5 and llama2-13b-chat to self-\\nbootstrap pipelines that outperform standard few-shot prompting (generally by\\nover 25% and 65%, respectively) and pipelines with expert-created demonstra-\\ntions (by up to 5–46% and 16–40%, respectively). On top of that, DSPy pro-\\ngrams compiled to open and relatively small LMs like 770M-parameter T5 and\\nllama2-13b-chat are competitive with approaches that rely on expert-written\\nprompt chains for proprietary GPT-3.5.\\nDSPy is available at https://github.com/stanfordnlp/dspy.\\n1 I NTRODUCTION\\nLanguage models (LMs) are enabling researchers to build NLP systems at higher levels of abstrac-\\ntion and with lower data requirements than ever before (Bommasani et al., 2021). This is fueling an\\nexploding space of “prompting” techniques—and lightweight finetuning techniques—for adapting\\nLMs to new tasks (Kojima et al., 2022), eliciting systematic reasoning from them (Wei et al., 2022;\\nWang et al., 2022b), andaugmenting them with retrieved sources (Guu et al., 2020; Lazaridou et al.,\\n2022; Khattab et al., 2022) or with tools (Yao et al., 2022; Schick et al., 2023). Most of these tech-\\nniques are explored in isolation, but interest has been growing in building multi-stage pipelines and\\nagents that decompose complex tasks into more manageable calls to LMs in an effort to improve\\nperformance (Qi et al., 2019; Khattab et al., 2021a; Karpas et al., 2022; Dohan et al., 2022; Khot\\net al., 2022; Khattab et al., 2022; Chen et al., 2022; Pourreza & Rafiei, 2023; Shinn et al., 2023).\\nUnfortunately, LMs are known to be sensitive to how they are prompted for each task, and this is\\nexacerbated in pipelines where multiple LM calls have to interact effectively. As a result, the LM\\n1\\narXiv:2310.03714v1  [cs.CL]  5 Oct 2023'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs[0].get_content()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ee9db5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "from llama_index.core import VectorStoreIndex, ServiceContext, StorageContext\n",
    "# from llama_index.core.node_parser import SimpleNodeParser\n",
    "\n",
    "\n",
    "def create_index(documents):\n",
    "\n",
    "    vector_store = QdrantVectorStore(client=client,\n",
    "                                     collection_name=collection_name)\n",
    "    \n",
    "\n",
    "    # # Configure the node parser with desired chunk size and overlap\n",
    "    # node_parser = SimpleNodeParser.from_defaults(chunk_size=512, chunk_overlap=50)\n",
    "\n",
    "    # # Create a service context with the custom node parser\n",
    "    # service_context = ServiceContext.from_defaults(node_parser=node_parser)\n",
    "\n",
    "    #By default, LlamaIndex splits documents into chunks of 1024 tokens with an overlap of 20 tokens\n",
    "    \n",
    "    #we configure storage settings by specifying the above vector_store as the storage backend.\n",
    "    storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "    \n",
    "    #Finally, we create an index by embedding each document in documents and storing it in the Qdrant vector store.\n",
    "    index = VectorStoreIndex.from_documents(documents,\n",
    "                                            storage_context=storage_context)\n",
    "    \n",
    "\n",
    "\n",
    "    return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "98a32541",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now, we will create the embeddings and store them in the Qdrant vector store.\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core import Settings\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "#embed_model as the default embedding model in Settings. This setting ensures that the same model is used throughout our RAG pipeline to maintain consistency in embedding generation.\n",
    "Settings.embed_model = embed_model\n",
    "\n",
    "index = create_index(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d7dc70f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x0000013CB1C4FC90>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None, show_progress_bar=False)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ba2b9290",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "\n",
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "aa7af7f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.core import PromptTemplate\n",
    "\n",
    "template = \"\"\"Context information is below:\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner,\n",
    "              incase you don't know the answer say 'I don't know!'\n",
    "            \n",
    "              Query: {query_str}\n",
    "        \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2074c761",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\huggingface_hub\\file_download.py:144: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\vijit_singh\\.cache\\huggingface\\hub\\models--cross-encoder--ms-marco-MiniLM-L-2-v2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
     ]
    }
   ],
   "source": [
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "\n",
    "rerank = SentenceTransformerRerank(\n",
    "    model=\"cross-encoder/ms-marco-MiniLM-L-2-v2\", \n",
    "    top_n=3\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "38087a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#engine should retrieve the top 10 most similar document chunks based on vector similarity to the query.\n",
    "query_engine = index.as_query_engine(similarity_top_k=10,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")\n",
    "\n",
    "response = query_engine.query(\"What exactly is DSPy?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "afe57b6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "DSPy stands for Declarative Signatures and Parameterized Significance, which refers to a programming model used for natural language processing (NLP) tasks. It's an approach that allows users to define the structure of their prompts or questions using natural language signatures, rather than writing explicit code for each prompt.\n",
       "\n",
       "In simpler terms, DSPy provides a way to abstract and automate the process of prompting Natural Language Models (LMs), such as those used in chatbots, text analysis, and other NLP tasks. This is achieved by defining a set of declarative instructions or signatures that specify how to transform input data into output results.\n",
       "\n",
       "These signatures are represented as tuples of fields, each with its own metadata, which include information about the field's purpose, description, and any optional constraints on its value. By using DSPy, developers can write concise and expressive code that defines their NLP tasks, rather than relying on explicit programming or scripting.\n",
       "\n",
       "The key features of DSPy include:\n",
       "\n",
       "1. Natural language signatures: Define prompts or questions as declarative instructions using natural language.\n",
       "2. Structured formatting: Use placeholders for fields to indicate the expected input format.\n",
       "3. Optional constraints: Specify limits on field values, such as integer types or specific data formats.\n",
       "4. Parameterized significance: Allow users to re-use and combine DSPy signatures across different tasks.\n",
       "\n",
       "Overall, DSPy provides a powerful tool for building and automating NLP workflows, making it easier to develop complex natural language processing tasks with minimal coding effort."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown, display\n",
    "\n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e877343d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

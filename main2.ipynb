{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac832bc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import nest_asyncio\n",
    "import qdrant_client\n",
    "\n",
    "from llama_index.core import Settings\n",
    "from llama_index.core import PromptTemplate\n",
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import StorageContext\n",
    "from llama_index.core import VectorStoreIndex, SimpleDirectoryReader\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.postprocessor import SentenceTransformerRerank\n",
    "from llama_index.embeddings.fastembed import FastEmbedEmbedding \n",
    "from llama_index.vector_stores.qdrant import QdrantVectorStore\n",
    "\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "648d35f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "03833741",
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = Ollama(model=\"llama3.2:1b\", request_timeout=120.0)\n",
    "\n",
    "embed_model = HuggingFaceEmbedding(model_name=\"BAAI/bge-large-en-v1.5\",\n",
    "                                   trust_remote_code=True)\n",
    "\n",
    "rerank = SentenceTransformerRerank(model=\"BAAI/bge-reranker-base\", top_n=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "80221dba",
   "metadata": {},
   "outputs": [],
   "source": [
    "Settings.embed_model = embed_model\n",
    "\n",
    "Settings.llm = llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86f2f77e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HuggingFaceEmbedding(model_name='BAAI/bge-large-en-v1.5', embed_batch_size=10, callback_manager=<llama_index.core.callbacks.base.CallbackManager object at 0x0000015AD69A77D0>, num_workers=None, max_length=512, normalize=True, query_instruction=None, text_instruction=None, cache_folder=None, show_progress_bar=False)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Settings.embed_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b9c4f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dir_path = './paul_graham/'\n",
    "\n",
    "loader = SimpleDirectoryReader(\n",
    "            input_dir = input_dir_path,\n",
    "            required_exts=[\".txt\"],\n",
    "            recursive=True\n",
    "        )\n",
    "docs = loader.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d0acd575",
   "metadata": {},
   "outputs": [],
   "source": [
    "client = qdrant_client.QdrantClient(host=\"localhost\", port=6333)\n",
    "\n",
    "vector_store = QdrantVectorStore(client=client,\n",
    "                                 collection_name=\"document_chat\")\n",
    "\n",
    "storage_context = StorageContext.from_defaults(vector_store=vector_store)\n",
    "\n",
    "index = VectorStoreIndex.from_documents(docs,\n",
    "                                        storage_context=storage_context)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc567982",
   "metadata": {},
   "outputs": [],
   "source": [
    "query_engine = index.as_query_engine(similarity_top_k=4,\n",
    "                                     node_postprocessors=[rerank])\n",
    "\n",
    "template = \"\"\"Context information is below.\n",
    "              ---------------------\n",
    "              {context_str}\n",
    "              ---------------------\n",
    "              Given the context information above I want you to think\n",
    "              step by step to answer the query in a crisp manner. Incase \n",
    "              you don't know the answer say 'I don't know!'.\n",
    "              \n",
    "              Query: {query_str}\n",
    "              \n",
    "              Answer:\"\"\"\n",
    "\n",
    "qa_prompt_tmpl = PromptTemplate(template)\n",
    "\n",
    "query_engine.update_prompts(\n",
    "    {\"response_synthesizer:text_qa_template\": qa_prompt_tmpl}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3fc8226d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The structure of funding startups in batches contributed to the success and growth of the Y Combinator program and the startups involved through several mechanisms:\n",
       "\n",
       "1. **Pooling resources**: Funding startups in batches allowed Y Combinator to pool its resources, expertise, and networks across multiple companies at once. This enabled it to provide more comprehensive support and mentorship to its portfolio companies.\n",
       "2. **Reducing risk**: By funding startups in groups rather than individuals, Y Combinator reduced the risk of losing a single investment. If one startup failed, the others could continue to grow and improve their chances of success.\n",
       "3. **Increasing efficiency**: Batching funding allowed Y Combinator to focus on managing its existing portfolio companies more efficiently. It could allocate resources, provide guidance, and make decisions in real-time without having to devote as much time to individual startups.\n",
       "4. **Encouraging collaboration**: Funding startups in batches fostered a culture of collaboration among the companies. Each startup received support from multiple Y Combinator employees, who shared their expertise and resources to help each other grow.\n",
       "5. **Identifying promising trends**: Batching funding enabled Y Combinator to identify emerging trends and areas of interest more effectively. By providing support to multiple startups in related fields or technologies, the organization could better understand what was working and what wasn't.\n",
       "\n",
       "These mechanisms combined to create a robust and supportive environment for startup growth, which contributed significantly to the success and growth of the Y Combinator program and its involved startups."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = query_engine.query(\"\"\"How did the structure of funding startups \n",
    "                                 in batches contribute to the success and \n",
    "                                 growth of the Y Combinator program and the\n",
    "                                 startups involved?\"\"\")\n",
    "                                 \n",
    "display(Markdown(str(response)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927bc642",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12596355",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "loader = DirectoryLoader(\"./paul_graham/\")\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1024, chunk_overlap=20)\n",
    "\n",
    "documents = loader.load_and_split(text_splitter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "83d3a3f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'lc': 1,\n",
       " 'type': 'constructor',\n",
       " 'id': ['langchain', 'schema', 'document', 'Document'],\n",
       " 'kwargs': {'page_content': 'How to Do Great Work\\n\\nJuly 2023\\n\\nIf you collected lists of techniques for doing great work in a lot of different fields, what would the intersection look like? I decided to find out by making it.\\n\\nPartly my goal was to create a guide that could be used by someone working in any field. But I was also curious about the shape of the intersection. And one thing this exercise shows is that it does have a definite shape; it\\'s not just a point labelled \"work hard.\"\\n\\nThe following recipe assumes you\\'re very ambitious.\\n\\nThe first step is to decide what to work on. The work you choose needs to have three qualities: it has to be something you have a natural aptitude for, that you have a deep interest in, and that offers scope to do great work.\\n\\nIn practice you don\\'t have to worry much about the third criterion. Ambitious people are if anything already too conservative about it. So all you need to do is find something you have an aptitude for and great interest in. [1]',\n",
       "  'metadata': {'source': 'paul_graham\\\\how_to_do_great_things.txt'},\n",
       "  'type': 'Document'}}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].to_json()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46ff7443",
   "metadata": {},
   "source": [
    "We will need three models here:\n",
    "\n",
    "- A generator model that generates the QA pairs based on the provided context.\n",
    "- An embedding to generate embeddings from raw text (will be used to retrieve & generate context).\n",
    "- A critic model for validating the generation process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6d4b40fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "generator_llm = Ollama(model=\"phi3:3.8b\")\n",
    "critic_llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(\n",
    "    model=\"nomic-embed-text\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b6f3661",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception in thread Thread-5:                                    \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\threading.py\", line 1038, in _bootstrap_inner\n",
      "    self.run()\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\executor.py\", line 96, in run\n",
      "    results = self.loop.run_until_complete(self._aresults())\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\nest_asyncio.py\", line 98, in run_until_complete\n",
      "    return f.result()\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\executor.py\", line 84, in _aresults\n",
      "    raise e\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\executor.py\", line 79, in _aresults\n",
      "    r = await future\n",
      "        ^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 605, in _wait_for_one\n",
      "    return f.result()  # May raise f.exception().\n",
      "           ^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\futures.py\", line 203, in result\n",
      "    raise self._exception.with_traceback(self._exception_tb)\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\asyncio\\tasks.py\", line 267, in __step\n",
      "    result = coro.send(None)\n",
      "             ^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\executor.py\", line 38, in sema_coro\n",
      "    return await coro\n",
      "           ^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\executor.py\", line 112, in wrapped_callable_async\n",
      "    return counter, await callable(*args, **kwargs)\n",
      "                    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\testset\\extractor.py\", line 49, in extract\n",
      "    results = await self.llm.generate(prompt=prompt, is_async=is_async)\n",
      "              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\llms\\base.py\", line 92, in generate\n",
      "    return await agenerate_text_with_retry(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 189, in async_wrapped\n",
      "    return await copy(fn, *args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 111, in __call__\n",
      "    do = await self.iter(retry_state=retry_state)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 153, in iter\n",
      "    result = await action(retry_state)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\_utils.py\", line 99, in inner\n",
      "    return call(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\__init__.py\", line 418, in exc_check\n",
      "    raise retry_exc.reraise()\n",
      "          ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\__init__.py\", line 185, in reraise\n",
      "    raise self.last_attempt.result()\n",
      "          ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 449, in result\n",
      "    return self.__get_result()\n",
      "           ^^^^^^^^^^^^^^^^^^^\n",
      "  File \"C:\\Users\\vijit_singh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\concurrent\\futures\\_base.py\", line 401, in __get_result\n",
      "    raise self._exception\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\tenacity\\asyncio\\__init__.py\", line 114, in __call__\n",
      "    result = await fn(*args, **kwargs)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\llms\\base.py\", line 177, in agenerate_text\n",
      "    result = await self.langchain_llm.agenerate_prompt(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 643, in agenerate_prompt\n",
      "    return await self.agenerate(\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 1018, in agenerate\n",
      "    output = await self._agenerate_helper(\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 882, in _agenerate_helper\n",
      "    raise e\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_core\\language_models\\llms.py\", line 866, in _agenerate_helper\n",
      "    await self._agenerate(\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 453, in _agenerate\n",
      "    final_chunk = await super()._astream_with_aggregation(\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 352, in _astream_with_aggregation\n",
      "    async for stream_resp in self._acreate_generate_stream(prompt, stop, **kwargs):\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 187, in _acreate_generate_stream\n",
      "    async for item in self._acreate_stream(\n",
      "  File \"c:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\langchain_community\\llms\\ollama.py\", line 305, in _acreate_stream\n",
      "    raise OllamaEndpointNotFoundError(\n",
      "langchain_community.llms.ollama.OllamaEndpointNotFoundError: Ollama call failed with status code 404.\n"
     ]
    },
    {
     "ename": "ExceptionInRunner",
     "evalue": "The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mExceptionInRunner\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 11\u001b[39m\n\u001b[32m      4\u001b[39m generator = TestsetGenerator.from_langchain(\n\u001b[32m      5\u001b[39m     generator_llm=generator_llm,\n\u001b[32m      6\u001b[39m     critic_llm=critic_llm,\n\u001b[32m      7\u001b[39m     embeddings=ollama_emb\n\u001b[32m      8\u001b[39m )\n\u001b[32m     10\u001b[39m distribution = {simple: \u001b[32m0.5\u001b[39m, reasoning: \u001b[32m0.25\u001b[39m, multi_context: \u001b[32m0.25\u001b[39m}\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m testset = \u001b[43mgenerator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate_with_langchain_docs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     12\u001b[39m \u001b[43m                                                 \u001b[49m\u001b[43mtest_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     13\u001b[39m \u001b[43m                                                 \u001b[49m\u001b[43mdistributions\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdistribution\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     14\u001b[39m \u001b[43m                                                 \u001b[49m\u001b[43mraise_exceptions\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\testset\\generator.py:175\u001b[39m, in \u001b[36mTestsetGenerator.generate_with_langchain_docs\u001b[39m\u001b[34m(self, documents, test_size, distributions, with_debugging_logs, is_async, raise_exceptions, run_config)\u001b[39m\n\u001b[32m    173\u001b[39m distributions = distributions \u001b[38;5;129;01mor\u001b[39;00m {}\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# chunk documents and add to docstore\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdocstore\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd_documents\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    176\u001b[39m \u001b[43m    \u001b[49m\u001b[43m[\u001b[49m\u001b[43mDocument\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_langchain_document\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdoc\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mdocuments\u001b[49m\u001b[43m]\u001b[49m\n\u001b[32m    177\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.generate(\n\u001b[32m    180\u001b[39m     test_size=test_size,\n\u001b[32m    181\u001b[39m     distributions=distributions,\n\u001b[32m   (...)\u001b[39m\u001b[32m    185\u001b[39m     run_config=run_config,\n\u001b[32m    186\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\testset\\docstore.py:215\u001b[39m, in \u001b[36mInMemoryDocumentStore.add_documents\u001b[39m\u001b[34m(self, docs, show_progress)\u001b[39m\n\u001b[32m    210\u001b[39m \u001b[38;5;66;03m# split documents with self.splitter into smaller nodes\u001b[39;00m\n\u001b[32m    211\u001b[39m nodes = [\n\u001b[32m    212\u001b[39m     Node.from_langchain_document(d)\n\u001b[32m    213\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m d \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.splitter.transform_documents(docs)\n\u001b[32m    214\u001b[39m ]\n\u001b[32m--> \u001b[39m\u001b[32m215\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madd_nodes\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m=\u001b[49m\u001b[43mshow_progress\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\vijit_singh\\Desktop\\Personal projects repo\\rag_app_lindex\\rag_test\\Lib\\site-packages\\ragas\\testset\\docstore.py:254\u001b[39m, in \u001b[36mInMemoryDocumentStore.add_nodes\u001b[39m\u001b[34m(self, nodes, show_progress)\u001b[39m\n\u001b[32m    252\u001b[39m results = executor.results()\n\u001b[32m    253\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m results:\n\u001b[32m--> \u001b[39m\u001b[32m254\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ExceptionInRunner()\n\u001b[32m    256\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, n \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(nodes):\n\u001b[32m    257\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m nodes_to_embed.keys():\n",
      "\u001b[31mExceptionInRunner\u001b[39m: The runner thread which was running the jobs raised an exeception. Read the traceback above to debug it. You can also pass `raise_exceptions=False` incase you want to show only a warning message instead."
     ]
    }
   ],
   "source": [
    "from ragas.testset.generator import TestsetGenerator\n",
    "from ragas.testset.evolutions import simple, reasoning, multi_context\n",
    "\n",
    "generator = TestsetGenerator.from_langchain(\n",
    "    generator_llm=generator_llm,\n",
    "    critic_llm=critic_llm,\n",
    "    embeddings=ollama_emb\n",
    ")\n",
    "\n",
    "distribution = {simple: 0.5, reasoning: 0.25, multi_context: 0.25}\n",
    "testset = generator.generate_with_langchain_docs(documents,\n",
    "                                                 test_size=10,\n",
    "                                                 distributions=distribution,\n",
    "                                                 raise_exceptions=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "46d08cf5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'testset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m test_df = \u001b[43mtestset\u001b[49m.to_pandas().dropna()\n",
      "\u001b[31mNameError\u001b[39m: name 'testset' is not defined"
     ]
    }
   ],
   "source": [
    "test_df = testset.to_pandas().dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a99beb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "66b03486",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "test_df = pd.read_csv(r\".\\paul_graham\\test_data_paul_graham.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "45ec0100",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a58ba214",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>question</th>\n",
       "      <th>contexts</th>\n",
       "      <th>ground_truth</th>\n",
       "      <th>evolution_type</th>\n",
       "      <th>metadata</th>\n",
       "      <th>episode_done</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>How did the shift to publishing on the web cha...</td>\n",
       "      <td>[\"Wow, I thought, there's an audience. If I wr...</td>\n",
       "      <td>The shift to publishing on the web changed the...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'paul_graham/what_i_worked_on.txt'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>How does criticizing a project as a \"toy\" rese...</td>\n",
       "      <td>[\"[9] You can't usually get paid for doing exa...</td>\n",
       "      <td>Criticizing a project as a 'toy' is similar to...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'paul_graham/how_to_do_great_thing...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>How did the structure of funding startups in b...</td>\n",
       "      <td>['The deal for startups was based on a combina...</td>\n",
       "      <td>Funding startups in batches allowed for conven...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'paul_graham/what_i_worked_on.txt'...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>How can exploring different topics help in gen...</td>\n",
       "      <td>[\"Talking or writing about the things you're i...</td>\n",
       "      <td>Exploring different topics can help in generat...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'paul_graham/how_to_do_great_thing...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>How does focusing consistently on something yo...</td>\n",
       "      <td>[\"The way to beat it is to stop occasionally a...</td>\n",
       "      <td>Great work happens by focusing consistently on...</td>\n",
       "      <td>simple</td>\n",
       "      <td>[{'source': 'paul_graham/how_to_do_great_thing...</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                                           question  \\\n",
       "0           0  How did the shift to publishing on the web cha...   \n",
       "1           1  How does criticizing a project as a \"toy\" rese...   \n",
       "2           2  How did the structure of funding startups in b...   \n",
       "3           3  How can exploring different topics help in gen...   \n",
       "4           4  How does focusing consistently on something yo...   \n",
       "\n",
       "                                            contexts  \\\n",
       "0  [\"Wow, I thought, there's an audience. If I wr...   \n",
       "1  [\"[9] You can't usually get paid for doing exa...   \n",
       "2  ['The deal for startups was based on a combina...   \n",
       "3  [\"Talking or writing about the things you're i...   \n",
       "4  [\"The way to beat it is to stop occasionally a...   \n",
       "\n",
       "                                        ground_truth evolution_type  \\\n",
       "0  The shift to publishing on the web changed the...         simple   \n",
       "1  Criticizing a project as a 'toy' is similar to...         simple   \n",
       "2  Funding startups in batches allowed for conven...         simple   \n",
       "3  Exploring different topics can help in generat...         simple   \n",
       "4  Great work happens by focusing consistently on...         simple   \n",
       "\n",
       "                                            metadata  episode_done  \n",
       "0  [{'source': 'paul_graham/what_i_worked_on.txt'...          True  \n",
       "1  [{'source': 'paul_graham/how_to_do_great_thing...          True  \n",
       "2  [{'source': 'paul_graham/what_i_worked_on.txt'...          True  \n",
       "3  [{'source': 'paul_graham/how_to_do_great_thing...          True  \n",
       "4  [{'source': 'paul_graham/how_to_do_great_thing...          True  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d561205",
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine is our rag app\n",
    "def generate_response(query_engine, question):\n",
    "    response = query_engine.query(question)\n",
    "    return {\n",
    "        \"answer\": response.response,\n",
    "        \"contexts\": [c.node.get_content() for c in response.source_nodes], #from original rag app we will pass ragas ques, it will return the context and answer\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cdb977b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 47/47 [44:18<00:00, 56.56s/it]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "test_questions = test_df[\"question\"].values\n",
    "\n",
    "responses = [generate_response(query_engine, q) for q in tqdm(test_questions)]\n",
    "\n",
    "dataset_dict = {\n",
    "    \"question\": test_questions,\n",
    "    \"answer\": [response[\"answer\"] for response in responses], # answer from original rag app\n",
    "    \"contexts\": [response[\"contexts\"] for response in responses], # context from original rag app\n",
    "    \"ground_truth\": test_df[\"ground_truth\"].values.tolist(), # ground truth from test_df(created by ragas)\n",
    "}\n",
    "\n",
    "ragas_eval_dataset = Dataset.from_dict(dataset_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2e451d15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_community.llms import Ollama\n",
    "\n",
    "from ragas import evaluate\n",
    "from ragas.metrics import (\n",
    "    faithfulness,\n",
    "    answer_correctness,\n",
    "    context_recall,\n",
    "    context_precision,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa0ce3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   2%|▏         | 3/188 [09:38<8:05:54, 157.59s/it] Failed to parse output. Returning None.\n",
      "Evaluating:   2%|▏         | 4/188 [11:27<7:05:09, 138.64s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   3%|▎         | 6/188 [13:47<4:52:49, 96.53s/it] Failed to parse output. Returning None.\n",
      "Evaluating:   4%|▎         | 7/188 [14:20<3:48:33, 75.76s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   5%|▍         | 9/188 [17:14<3:53:26, 78.25s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   5%|▌         | 10/188 [20:52<5:59:55, 121.32s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   6%|▌         | 11/188 [21:12<4:26:32, 90.35s/it] Failed to parse output. Returning None.\n",
      "Evaluating:   7%|▋         | 13/188 [22:12<2:52:56, 59.29s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   7%|▋         | 14/188 [22:17<2:04:34, 42.96s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   9%|▊         | 16/188 [25:10<3:10:57, 66.62s/it]Failed to parse output. Returning None.\n",
      "Evaluating:   9%|▉         | 17/188 [26:01<2:56:32, 61.94s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  10%|█         | 19/188 [29:23<3:39:07, 77.79s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  11%|█         | 20/188 [32:55<5:30:35, 118.07s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  12%|█▏        | 22/188 [35:28<4:24:46, 95.70s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  12%|█▏        | 23/188 [37:32<4:46:51, 104.31s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  13%|█▎        | 24/188 [41:31<6:35:06, 144.55s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  13%|█▎        | 25/188 [42:51<5:40:27, 125.32s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  14%|█▍        | 26/188 [43:30<4:28:28, 99.43s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  15%|█▌        | 29/188 [45:08<2:21:26, 53.38s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  16%|█▌        | 30/188 [48:04<3:57:24, 90.15s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  17%|█▋        | 32/188 [52:35<4:32:46, 104.91s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  19%|█▊        | 35/188 [54:52<2:41:35, 63.37s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  19%|█▉        | 36/188 [57:09<3:36:00, 85.27s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  20%|█▉        | 37/188 [58:05<3:12:39, 76.56s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  21%|██▏       | 40/188 [1:06:32<5:01:44, 122.33s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  22%|██▏       | 42/188 [1:13:16<6:05:16, 150.11s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  23%|██▎       | 43/188 [1:14:40<5:14:31, 130.15s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  23%|██▎       | 44/188 [1:15:34<4:17:59, 107.50s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  24%|██▍       | 45/188 [1:16:41<3:47:10, 95.32s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  24%|██▍       | 46/188 [1:17:18<3:04:25, 77.93s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  25%|██▌       | 47/188 [1:19:52<3:56:27, 100.62s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  26%|██▌       | 49/188 [1:24:20<4:13:23, 109.38s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  27%|██▋       | 50/188 [1:25:37<3:48:49, 99.49s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  27%|██▋       | 51/188 [1:32:35<7:25:23, 195.06s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  28%|██▊       | 52/188 [1:36:51<8:03:32, 213.32s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  29%|██▊       | 54/188 [1:39:38<5:32:06, 148.70s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  29%|██▉       | 55/188 [1:40:06<4:09:20, 112.48s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  31%|███       | 58/188 [1:45:07<4:06:22, 113.71s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  31%|███▏      | 59/188 [1:45:58<3:24:16, 95.01s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  32%|███▏      | 60/188 [1:48:09<3:45:33, 105.73s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  33%|███▎      | 62/188 [1:49:56<2:48:19, 80.16s/it] Failed to parse output. Returning None.\n",
      "Evaluating:  36%|███▌      | 68/188 [2:08:57<6:38:08, 199.07s/it]Failed to parse output. Returning None.\n",
      "Evaluating:  37%|███▋      | 69/188 [2:09:52<5:09:12, 155.90s/it]"
     ]
    }
   ],
   "source": [
    "metrics = [faithfulness, answer_correctness,\n",
    "           context_recall, context_precision]\n",
    "\n",
    "critic_llm = Ollama(model=\"llama3.2:1b\")\n",
    "\n",
    "ollama_emb = OllamaEmbeddings(model=\"nomic-embed-text\")\n",
    "\n",
    "evaluation_result = evaluate(\n",
    "    llm=critic_llm,\n",
    "    embeddings=ollama_emb,\n",
    "    dataset=ragas_eval_dataset,\n",
    "    metrics=metrics\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c00e163",
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_scores_df = pd.DataFrame(evaluation_result.scores)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
